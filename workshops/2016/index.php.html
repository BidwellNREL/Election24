


<!doctype html>
<html class="no-js" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1.0" /><title>HPC Sys Pros</title><link rel="stylesheet" href="http://hpcsyspros.lsu.edu/2016//css/app.css" /><style>
          .smallCaps {
               font-variant: small-caps;
               text-decoraton: bolder;
               font-size: 125%;
               color: #000000;
               }

           .fullwidth {
              width: 90%;
              margin-left: auto;
              margin-right: auto;
              max-width: initial;
           }
           div.footer {t: right;
               font-size: 75%;
               font-style: italic;
               id gray;
               line-height: 100%;
               color: #000000;
           }       
           div.scusa {
               background-color: #FFFFFF;
               color: #000000;
               line-height: 125%;
               font-size: 90%;
               margin-right: 5%;
               margin-left: 5%;
               padding-left: 0.5em;
               padding-right: 0.5em;
           }
           li:hover ul ul, li:hover ul ul ul, li.sfhover ul ul, li.sfhover ul ul ul {
               left: -999em;
               background-color: #000000;
               border: 1px solid #FFFFFF;
               padding: 2px;
           }
           h4 {
              background-color: #cccccc;
              font-variant: small-caps;
           }
           img {
              width: 100%;
              height: auto;
              overflow: hidden;
           }
          </style></head><body>
<div class="row fullwidth">
<div id="nav">
<ul>

<div>
<nav class="top-bar" data-topbar role="navigation">
  <section class="top-bar-section">

    <!-- Left Nav Section --> 
    <ul class="left">
      <li><a href="http://hpcsyspros.lsu.edu/2016//index.php#info">Information</a></li>
      <li><a href="http://hpcsyspros.lsu.edu/2016//index.php#invite">Invited Paper</a></li>
      <li><a href="http://hpcsyspros.lsu.edu/2016//index.php#schedule">Schedule</a></li>
      <li><a href="http://hpcsyspros.lsu.edu/2016//index.php#topic">Topics</a></li>
      <li><a href="http://hpcsyspros.lsu.edu/2016//index.php#soon">Information</a></li>
      <li><a href="http://hpcsyspros.lsu.edu/2016//index.php#org">Organizing Committee</a></li>
      <li><a href="http://hpcsyspros.lsu.edu/2016//index.php#prog">Program Committee</a></li>
      <li><a href="http://hpcsyspros.lsu.edu/2016//papers.php">Call for Papers</a></li>
      <li><a href="http://hpcsyspros.lsu.edu/2016//index.php#contact">Contact</a></li>
      <li><a href="http://hpcsyspros.lsu.edu/2016//index.php#links">Links</a></li>
    </ul>
   </li>
  </section>
</nav>
</div>
</div>
</div>
<div class="row fullwidth">
<div class="grid-frame">
<div class="large-12 medium-12 small-12 column">
<center><h2>Inaugural HPC Systems Professionals Workshop</h2></center>
<center><h2>HPCSYSPROS16</h2></center>
<center><h2>November 14, 2016, Salt Lake City, Utah</h2></center>
</div>
<div class="large-4 medium-4 small-4 column">&nbsp;</div>
<div class="large-3 medium-3 small-3 column"><a href="http://sc16.supercomputing.org"><img src="../images/SC16-logo.png" height="129" width="256" alt="SC16 Logo" align="middle"></a></div>
<div class="large-2 medium-2 small-2 column">&nbsp;</div>
<div class="large-4 medium-4 small-4 column">&nbsp;</div>
<div class="large-12 medium-12 small-12 column">
<center><h2>Held in conjunction with <a href="http://sc16.supercomputing.org">SC16</a><!-- , in cooperation with <a href="http://www.sighpc.org">SIGHPC</a> -->.</h2></center>
</div>
</div>
</div>
<div id="mainBody" class="scusa">
<!-- Content goes here. -->
<a name="info"></a>
<h4>&nbsp;Quick Information</h4>
<p>
In order to meet the demands of high performance computing (HPC) researchers, large-scale computational and storage machines
require many staff members who design, install, and maintain these systems. These HPC systems professionals include system
engineers, system administrators, network administrators, storage administrators and operations staff all who face problems that
are specific to high performance systems.
<p>
The <!-- SIGHPC --> Systems Professionals Workshop intends to be a platform for discussing the unique challenges that come from
supporting large-scale, high performance systems. We are soliciting submissions that speak directly to the state of the practice
of standing up and operating high performance systems with an emphasis on solutions that can be implemented by systems staff at
other institutions.
<p>
We will be presenting the <i>Inaugural HPC Systems Professionals Workshop</i> at <a href="https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=0ahUKEwjx1cXCo-nLAhVG32MKHWYrARoQFggdMAA&url=http%3A%2F%2Fsc16.supercomputing.org%2F&usg=AFQjCNECmFrv_vABgGm4LNikWnCtnFIVeQ&sig2=1SBZoP2UJO-lGpzyy5u3eQ">SC 16</a> on Monday afternoon (November 14, 2016).
</p>
<p>
<a name="invite"></a>
<H4>&nbsp;Invited Paper: HPC Systems Acceptance: Controlled Chaos</h4>
<p>
<p>
<table border='0'>
<tr>
<td style="vertical-align: top"><img src='images/Paul_Peltz.jpg' alt='picture of Paul Peltz'><br>Paul Peltz</td>
<td>
<p>
System acceptance is critical when deploying HPC resources from small clusters to large supercomputers. Each type of system
has its own challenges and having a well established and proven test, acceptance, and integration plan is an invaluable
tool to the site and the vendor. The topic of systems acceptance is quite broad. This talk will be mostly focused on the
system?s software and hardware components.
<p>
Biography:<br>
Paul Peltz is a Scalable Systems Engineer in the High Performance Computing
Division at Los Alamos National Laboratories where he helps procure, test,
and integrate the new HPC systems into production.  He has almost 20 years
of experience collaborating with vendors to evaluate and integrate
pre-release hardware and software.  Currently he is the systems technical
lead on the Trinity project and is actively working with Cray to deploy
their largest XC system to date, a 110 cabinet XC40 named Trinity.
<p>
</td>
</tr>
</table>

<!--
<p>
The complete paper may be viewed <a href="HPCSYSPROS16_Paper_5_Peltz.pdf">here</a> or you can see it
and <a href="https://scholarworks.iu.edu/dspace/handle/2022/21083">IUScholarWorks</a>.
-->
<p>
<a name="schedule"></a>
<h4>&nbsp;Schedule</h4>
<p>
<table border='1'>
<tr><th>Start</th><th>End</th><th>Description</th>
<!-- 
<th>Paper</th> 
<th>IUScholarWorks</th>
-->
<th>Slides</th></tr>
<tr><td>2:00 PM</td><td>2:05</td><td colspan="3">Welcome</td></tr>
<tr><td>2:05 PM</td><td>3:00 PM</td><td colspan="3">Panel Discussion: Recruiting and Professional Development of HPC Systems Professionals</td></tr>
<tr><td>3:00 PM</td><td>3:30 PM</td><td colspan="3">Break</td></tr>
<tr><td>3:30 PM</td><td>3:50 PM</td><td>Invited Paper: <A href="https://scholarworks.iu.edu/dspace/handle/2022/21083">HPC Systems Acceptance: Controlled Chaos</a> by Paul Peltz</td>
<!--
<td><a href="HPCSYSPROS16_Paper_5_Peltz.pdf">PDF</a></td>
-->
<td><a href="slides/HPC_Systems_Acceptance_Controlled_Chaos.pdf">Slides(pdf)</a></td>
</tr>
<!--
<tr><td>3:50 PM</td><td>4:00 PM</td><td>Q&amp;A</td></tr>
-->
<tr><td>4:00 PM</td><td>4:15 PM</td><td>Paper: <a href="https://scholarworks.iu.edu/dspace/handle/2022/21078">Account Management on Large-Scale HPC</a> by Brett Bode</td>
<td><a href="slides/Account_Management_on_a_Large-Scale_HPC_Resource.pptx">Slides(pptx)</a></td>
<!--
<td><a href="HPCSYSPROS16_Paper_2_Bode.pdf">PDF</a></td>
<td><a href="https://scholarworks.iu.edu/dspace/handle/2022/21078">IU</a></td>
<td><a href="slides/Account_Management_on_a_Large-Scale_HPC_Resource.pptx">Slides(pptx)</a></td>
-->
</tr>
<!--
<tr><td>3:50 PM</td><td>4:00 PM</td><td>Q&amp;A</td></tr>
-->
<tr><td>4:20 PM</td><td>4:35 PM</td><td>Paper: <a href="https://scholarworks.iu.edu/dspace/handle/2022/21082">Cluster Computing with OpenHPC</a> by Karl Schulz</td>
<td><a href="slides/OpenHPC-hycsyspro-overview.pdf">Slides(pdf)</a></td>
<!--
<td><a href="HPCSYSPROS16_Paper_4_Schulz.pdf">PDF</a></td>
<td><a href="https://scholarworks.iu.edu/dspace/handle/2022/21082">IU</a></td>
-->
</tr>
<!--
<tr><td>3:50 PM</td><td>4:00 PM</td><td>Q&amp;A</td></tr>
-->
<tr><td>4:40 PM</td><td>4:55 PM</td><td>Paper: <a href="https://scholarworks.iu.edu/dspace/handle/2022/21077">Increasing HPC Resiliency Leads to Greater Productivity</a> by Roger Moye</td>
<td><a href="slides/Increasing_HPC_Resiliency_Leads_to_Greater_Productivity.pdf">Slides(pdf)</a></td>
<!--
<td><a href="HPCSYSPROS16_Paper_1_Moye.pdf">PDF</a></td>
<td><a href="https://scholarworks.iu.edu/dspace/handle/2022/21077">IU</a></td>
<td><a href="slides/Increasing_HPC_Resiliency_Leads_to_Greater_Productivity.pdf">Slides(pdf)</a></td>
-->
</tr>
<!--
<tr><td>3:50 PM</td><td>4:00 PM</td><td>Q&amp;A</td></tr>
-->
<tr><td>5:00 PM</td><td>5:15 PM</td><td>Paper: <a href="https://scholarworks.iu.edu/dspace/handle/2022/21080">Blue Waters Resource Management and Job Scheduling Best Practices</a> by Jeremy Enos</td>
<td><a href="slides/Blue_Waters_Resource_Management_and_Job_Scheduling_Best_Practices.pptx">Slides(pptx)</a></td>
<!--
<td><a href="HPCSYSPROS16_Paper_3_Islam.pdf">PDF</a></td>
<td><a href="https://scholarworks.iu.edu/dspace/handle/2022/21080">IU</a></td>
<td><a href="slides/Blue_Waters_Resource_Management_and_Job_Scheduling_Best_Practices.pptx">Slides(pptx)</a></td>
-->
</tr>
<!--
<tr><td>3:50 PM</td><td>4:00 PM</td><td>Q&amp;A</td></tr>
-->
<tr><td>5:20 PM</td><td>5:30 PM</td><td colspan="3">Wrap Up</td></tr>
</table>
<b>Panel Members</b><br>
<table border='1'>
<tr><th>Member</th><th>Employer</th><th>Degree</th></tr>
<tr><td>Karl Schulz</td><td>Intel</td><td>PhD</td></tr>
<tr><td>Dane Skow</td><td>University of Wyoming</td><td>PhD</td></tr>
<tr><td>Paul Brenner</td><td>Notre Dame</td><td>PhD</td></tr>
<tr><td>Andree Jacobson</td><td>New Mexico Consortium</td><td>MS</td></tr>
</td></tr>
</table>
<a name="topics"></a>
<H4>&nbsp;Topics of Interest</h4>
<p>
Here are some topics of interest for this group. Note that these are here
to indicate direction, not to disallow other related topics.
<p>
<ul>
    <li>Cluster, configuration, or software management
    <li>Performance tuning/Benchmarking
    <li>Resource manager and job scheduler configuration
    <li>Monitoring/Mean-time-to-failure/ROI/Resource utilization
    <li>Virtualization/Clouds
    <li>Designing and troubleshooting HPC interconnects
    <li>Designing and maintaining HPC storage solutions
    <li>Cybersecurity and data protection
    <li>Cluster storage
</ul>
<p>
Example paper ideas might be:
<ul>
	<li>Best practices for job scheduler configuration 
	<li>Advantages of cluster automation
	<li>Managing software on HPC clusters
</ul>
<p>
<a name="soon"></a>
<h4>&nbsp;Information</h4>
<p>
<ul>
    <li><a href="http://hpcsyspros.lsu.edu/2016//papers.php">Call for Papers</a>
    <li>Calendar of events
<table border='1' cellpadding='2'>
<tr><th>Event</th><th>Date</th></tr>
<tr><td>Submission Deadline</td><td>August 26th</td></tr>
<tr><td>Acceptance Notifications</td><td>September 30th (<b>updated</b>)</td></tr>
<tr><td>Camera Ready Papers</td><td>October 21st (<b>updated</b>)</td></tr>
<tr><td>Inaugural HPC Systems Professionals Workshop</td><td>November 14th</td></tr>
</table>
</ul>
</p>
<a name="org"></a>
<h4>&nbsp;Organizing Committee</h4>
<p>
<table border='1'>
<tr><th>Position</th><th>Name</th><th>Affiliation</th></tr>
<tr><td>Chair</td><td>Randy Herban</td><td>Purdue University</td></tr>
<tr><td>Vice Chair</td><td>Isaac Traxler</td><td><a href="http://hpc.lsu.edu/">LSU HPC</a><br><a href="http://hpc.loni.org/">LONI HPC</a></tr>
<tr><td>Program Committee Chair</td><td>Jenett Tillotson</td><td>Indiana University</td></tr>
<tr><td>Organizing Committee</td>
    <td>William Scullin<br>Stephen Lien Harrell<br>Robert Ping<br>Prentice Bisbal<br>Henry Neeman</td>
    <td>Argonne National Laboratory<br>Purdue University<br>Indiana University<br>Princeton Plasma Physics Laboratory<br>University of Oklahoma</td></tr>
</table>
<a name="prog"></a>
<h4>&nbsp;Program Committee</h4>
<p>
<table border='1'>
<tr><th>Name</th><th>Affiliation</th></tr>
<tr><td>Brett Bode</td><td>NCSA</td></tr>
<tr><td>Matt Ezell</td><td>Oak Ridge National Labs</td></tr>
<tr><td>Pat Finnegan</td><td>Purdue University</td></tr>
<tr><td>Stephen Harrell</td><td>Purdue University</td></tr>
<tr><td>Randy Herban</td><td>Purdue University</td></tr>
<tr><td>Sharan Kalwani</td><td>Michigan State University</td></tr>
<tr><td>HonWai Leong</td><td>IBM/NCSA</td></tr>
<tr><td>Scott McMillan</td><td>NVIDIA</td></tr>
<tr><td>Henry Neeman</td><td>University of Oklahoma</td></tr>
<tr><td>Jeff Raymond</td><td>University of Pittsburgh</td></tr>
<tr><td>Randy Schauer</td><td>Ratheon</td></tr>
<tr><td>Jenett Tillotson</td><td>Indiana University</td></tr>
<tr><td>Isaac Traxler</td><td><a href="http://hpc.lsu.edu/">LSU HPC</a><br><a href="http://hpc.loni.org/">LONI HPC</a></tr>
<tr><td>George Turner</td><td>Indiana University</td></tr>
</table>
<a name="papers"></a>
<H4><a href="http://hpcsyspros.lsu.edu//papers.php">Call for Papers</a></h4>
<p>The <a href="http://hpcsyspros.lsu.edu/2016//papers.php">Call for papers is live</a>. We are
looking for papers related to HPC admining. <u><b>Our submission deadline has been extended to August 26th, 2016.</b></u>
<p>
<a name="contact"></a>
<h4>&nbsp;Contact Information</h4>
<p>
Please send email to <a href="mailto:hpcsyspromail@hpcsyspros.lsu.edu">hpcsyspros</a>.
<a name="links"></a>
<h4>&nbsp;Links</h4>
<ul>
    <li><a href="http://emac3.hpc.lsu.edu/mailman/listinfo/sc-admin">HPC Sysadmin Mailing List</a> - you should join!
<!--
    <li><a href=""></a>
    <li><a href=""></a>
-->
</ul>

</div><div class="footer" align="center">The statements and opinions included in these pages are those of the  only.<br>&copy; 2016 SYSPROS.</div> </body> </html>